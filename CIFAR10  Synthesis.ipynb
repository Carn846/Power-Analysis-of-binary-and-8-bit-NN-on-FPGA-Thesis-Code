{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49454ccd",
   "metadata": {},
   "source": [
    "# CIFAR - 10 Network Synthesis\n",
    "This code needs to be run within the FINN docker. This code will go through the synthesis of trained networks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "454f73fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Amount of time that the BNN will have to train is: 0.001 hours\n",
      "Training on: cuda:0\n",
      "Training Class initalised at: 2023-10-07 04:42:34.891804\n",
      "loading dataset: CIFAR10\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset loaded\n",
      "Network Accuracy: 48\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "## Load the network trainer script to get a reading on the accuracy of the network before the full script is run\n",
    "from NetworkTrainer import Networktrainer\n",
    "\n",
    "trinary = False # Change to the type of network you want to synth\n",
    "folding_amount = 50000 # Manually set the folding amount\n",
    "if trinary:\n",
    "    from CIFAR10 import Lenet5v8Tri\n",
    "    Network = Lenet5v8Tri()\n",
    "    inputVector = (1,3,32,32)\n",
    "    network_name = \"TrinaryLenet\" # Allow for easy changing of the network name\n",
    "    PATH = f'./Trained Networks/CIFAR-10 10 hours/Lenetv8 Trinary.pth' # Path to the trained network\n",
    "    Network.load_state_dict(torch.load(PATH)) # THIS WAS COMMENTED OUT\n",
    "    # Checking to see if the network loaded acutally has been trained\n",
    "    # Initialise the network trainer\n",
    "    bnnTrainTime = 0.02 # will leave hard coded in.\n",
    "    Trainer = Networktrainer(bnnTrainTime)\n",
    "    Trainer.load_dataset(\"CIFAR10\")\n",
    "    with open('dumy.txt','w') as dummyLogFile:\n",
    "        print(f'Network Accuracy: {Trainer.Test_Accuracy(Network,dummyLogFile)}')\n",
    "    Network.to('cpu')\n",
    "else:\n",
    "    # the 8 bit one\n",
    "    from CIFAR10 import Lenet5v9 #Lenet5v4,Lenet5v8Tri, Lenet5v5withBias, Lenet5v6withoutBias,Lenet5v7withBias,Quant8BitFCMNISTOld,Lenet5v9\n",
    "    quantisation = 9\n",
    "    Network = Lenet5v9()\n",
    "    inputVector = (1,3,32,32)\n",
    "    network_name = f\"Lenet5v9 v10_folding{folding_amount}\"  # Allow for easy changing of the network name\n",
    "    PATH = f'./Trained Networks/CIFAR-10 10 hours/Lenetv9 8-bit.pth' # Path to the trained network\n",
    "    Network.load_state_dict(torch.load(PATH)) # THIS WAS COMMENTED OUT\n",
    "    # Checking to see if the network loaded actually has been trained\n",
    "    # Initialise the network trainer\n",
    "    bnnTrainTime = 0.02 # will leave hard coded in.\n",
    "    Trainer = Networktrainer(0.001)\n",
    "    Trainer.load_dataset(\"CIFAR10\")\n",
    "    with open('dumy.txt','w') as dummyLogFile:\n",
    "        print(f'Network Accuracy: {Trainer.Test_Accuracy(Network,dummyLogFile)}')\n",
    "    Network.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d21f0b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 4324\n"
     ]
    }
   ],
   "source": [
    "# Determine the total number of trainable parameters\n",
    "total_params = sum(p.numel() for p in Network.parameters())\n",
    "print(f\"Number of parameters: {total_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cc148e",
   "metadata": {},
   "source": [
    "## Converting model into Qonnx\n",
    "Will need to export the model into a qonnx version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7fbd658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.util.basic import make_build_dir\n",
    "from finn.util.visualization import showInNetron\n",
    "import os\n",
    "\n",
    "import onnx\n",
    "from finn.util.test import get_test_model_trained\n",
    "import brevitas.onnx as bo\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "from qonnx.transformation.fold_constants import FoldConstants\n",
    "from qonnx.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames, RemoveStaticGraphInputs\n",
    "    \n",
    "build_dir = f'{os.getcwd()}/TestSynth' # Will allow a record of the Netron Models to be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b64c8aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julien/finn/deps/qonnx/src/qonnx/core/modelwrapper.py:93: UserWarning: Some old-style domain attributes were automatically converted to new-style,\n",
      "                i.e. domain=finn to domain=qonnx.custom_op.<general|fpgadataflow|...>\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Saving the network - taken from the demo\n",
    "bo.export_finn_onnx(Network, inputVector, build_dir + f\"/{network_name}_export.onnx\")\n",
    "model = ModelWrapper(build_dir + f\"/{network_name}_export.onnx\")\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(FoldConstants())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "model = model.transform(RemoveStaticGraphInputs())\n",
    "model.save(build_dir + f\"/{network_name}_tidy.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40913b0a",
   "metadata": {},
   "source": [
    "Display the imported qonnx model. No operations have taken place at the moment besides inital set up seen in last block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fd19661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving '/home/julien/finn/notebooks/Thesis/TestSynth/Lenet5v9 v10_folding50000_tidy.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fceeb96aa60>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(build_dir + f\"/{network_name}_tidy.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adebd789",
   "metadata": {},
   "source": [
    "## The pre and post processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "273c4399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just loading all the used modules\n",
    "from finn.util.pytorch import ToTensor\n",
    "from qonnx.transformation.merge_onnx_models import MergeONNXModels\n",
    "from qonnx.core.datatype import DataType\n",
    "from qonnx.transformation.insert_topk import InsertTopK\n",
    "from qonnx.transformation.infer_datatypes import InferDataTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a5c35ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Although not doing preprocessing will still do the model calls so that if you do use it you can just insert\n",
    "# your instructions here.\n",
    "model = ModelWrapper(build_dir+f\"/{network_name}_tidy.onnx\")\n",
    "# add input quantization annotation: UINT8 for all BNN-PYNQ models\n",
    "global_inp_name = model.graph.input[0].name\n",
    "model.set_tensor_datatype(global_inp_name, DataType[\"UINT8\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d5dc0b",
   "metadata": {},
   "source": [
    "### Post Processing,\n",
    "Inserting a topK layer that will allow the classification to pick a label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2266f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# postprocessing: insert Top-1 node at the end\n",
    "model = model.transform(InsertTopK(k=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6684278",
   "metadata": {},
   "source": [
    "### Tidy up the model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "587f68c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/home/julien/finn/notebooks/Thesis/TestSynth/Lenet5v9 v10_folding50000_pre_post.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fceeb977130>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tidy-up again\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(FoldConstants())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "model = model.transform(InferDataTypes())\n",
    "model = model.transform(RemoveStaticGraphInputs())\n",
    "model.save(build_dir+f\"/{network_name}_pre_post.onnx\")\n",
    "# Show the network again\n",
    "showInNetron(build_dir+f\"/{network_name}_pre_post.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5222ef70",
   "metadata": {},
   "source": [
    "### Streamlining and lowering layers\n",
    "This process is highly dependent on the topography of the network. As such it will differ from each type of network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bae87db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julien/finn/deps/qonnx/src/qonnx/transformation/infer_data_layouts.py:119: UserWarning: Assuming 4D input is NCHW\n",
      "  warnings.warn(\"Assuming 4D input is NCHW\")\n"
     ]
    }
   ],
   "source": [
    "from finn.transformation.streamline import Streamline\n",
    "import finn\n",
    "from qonnx.transformation.lower_convs_to_matmul import LowerConvsToMatMul\n",
    "from qonnx.transformation.bipolar_to_xnor import ConvertBipolarMatMulToXnorPopcount\n",
    "import finn.transformation.streamline.absorb as absorb\n",
    "import finn.transformation.streamline.reorder as reorder\n",
    "from finn.transformation.streamline.reorder import MakeMaxPoolNHWC, MoveScalarLinearPastInvariants\n",
    "from qonnx.transformation.infer_data_layouts import InferDataLayouts\n",
    "from qonnx.transformation.general import RemoveUnusedTensors\n",
    "\n",
    "model = ModelWrapper(build_dir + f\"/{network_name}_pre_post.onnx\")\n",
    "\n",
    "model = model.transform(absorb.AbsorbSignBiasIntoMultiThreshold())\n",
    "model = model.transform(MoveScalarLinearPastInvariants())\n",
    "model = model.transform(Streamline())\n",
    "model = model.transform(LowerConvsToMatMul())\n",
    "\n",
    "model = model.transform(MakeMaxPoolNHWC())\n",
    "model = model.transform(absorb.AbsorbTransposeIntoMultiThreshold())\n",
    "model = model.transform(ConvertBipolarMatMulToXnorPopcount())\n",
    "\n",
    "model = model.transform(Streamline())\n",
    "# absorb final add-mul nodes into TopK\n",
    "model = model.transform(absorb.AbsorbScalarMulAddIntoTopK())\n",
    "model = model.transform(InferDataLayouts())\n",
    "model = model.transform(RemoveUnusedTensors())\n",
    "# Gets rid of the repeated Transposes\n",
    "model = model.transform(absorb.AbsorbConsecutiveTransposes())\n",
    "model = model.transform(Streamline())\n",
    "\n",
    "model.save(build_dir+f\"/{network_name}_streamlined.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46d918c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/home/julien/finn/notebooks/Thesis/TestSynth/Lenet5v9 v10_folding50000_streamlined.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fceeb7e00a0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(build_dir+f\"/{network_name}_streamlined.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d748afe",
   "metadata": {},
   "source": [
    "### Converting the layers into the HW equivalent\n",
    "\n",
    "It is this stage which will be the hardest. I will need to ensure that each node is able to be converted a compatable version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6da0e145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/home/julien/finn/notebooks/Thesis/TestSynth/Lenet5v9 v10_folding50000_hls.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fceeb7edbe0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import finn.transformation.fpgadataflow.convert_to_hls_layers as to_hls\n",
    "from finn.transformation.fpgadataflow.create_dataflow_partition import (\n",
    "    CreateDataflowPartition\n",
    ")\n",
    "import finn.builder.build_dataflow \n",
    "from finn.transformation.move_reshape import RemoveCNVtoFCFlatten\n",
    "from qonnx.custom_op.registry import getCustomOp\n",
    "from qonnx.transformation.infer_data_layouts import InferDataLayouts\n",
    "\n",
    "# choose the memory mode for the MVTU units, decoupled or const\n",
    "mem_mode = \"decoupled\" # smaller memory foot print. Longer synth times use 'const'\n",
    "\n",
    "model = ModelWrapper(build_dir + f\"/{network_name}_streamlined.onnx\")\n",
    "model = model.transform(to_hls.InferBinaryMatrixVectorActivation(mem_mode))\n",
    "model = model.transform(to_hls.InferQuantizedMatrixVectorActivation(mem_mode))\n",
    "\n",
    "# TopK to LabelSelect\n",
    "model = model.transform(to_hls.InferLabelSelectLayer())\n",
    "# input quantization (if any) to standalone thresholding\n",
    "model = model.transform(to_hls.InferThresholdingLayer())\n",
    "model = model.transform(to_hls.InferConvInpGen())\n",
    "model = model.transform(to_hls.InferStreamingMaxPool())\n",
    "# get rid of Reshape(-1, 1) operation between hlslib nodes\n",
    "model = model.transform(RemoveCNVtoFCFlatten()) # comment out when not using any conv layers\n",
    "# get rid of Tranpose -> Tranpose identity seq\n",
    "# Deal with the max pool between the conv and fc layers\n",
    "model = model.transform(MakeMaxPoolNHWC())\n",
    "model = model.transform(absorb.AbsorbConsecutiveTransposes())\n",
    "model = model.transform(to_hls.InferStreamingMaxPool())\n",
    "\n",
    "\n",
    "# infer tensor data layouts\n",
    "model = model.transform(InferDataLayouts())\n",
    "model.save(build_dir+f\"/{network_name}_hls.onnx\")\n",
    "showInNetron(build_dir+f\"/{network_name}_hls.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50b0f641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partitioning the network\n",
    "parent_model = model.transform(CreateDataflowPartition())\n",
    "parent_model.save(build_dir + f\"/{network_name}_dataflow_parent.onnx\")\n",
    "sdp_node = parent_model.get_nodes_by_op_type(\"StreamingDataflowPartition\")[0]\n",
    "#print(sdp_node)\n",
    "sdp_node = getCustomOp(sdp_node)\n",
    "dataflow_model_filename = sdp_node.get_nodeattr(\"model\")\n",
    "# save the dataflow partition with a different name for easier access\n",
    "dataflow_model = ModelWrapper(dataflow_model_filename)\n",
    "dataflow_model.save(build_dir + f\"/{network_name}_dataflow_model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9c61e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/home/julien/finn/notebooks/Thesis/TestSynth/Lenet5v9 v10_folding50000_dataflow_model.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fcfee0a0070>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(build_dir + f\"/{network_name}_dataflow_model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fc5cacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/home/julien/finn/notebooks/Thesis/TestSynth/Lenet5v9 v10_folding50000_dataflow_parent.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fcfee09d3d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(build_dir + f\"/{network_name}_dataflow_parent.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ba7bda",
   "metadata": {},
   "source": [
    "# Folding the network\n",
    "Uses the c++ synthesis tool to determine folding settings. Not optimal but will be procedual for this thesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5475ff88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julien/finn/src/finn/transformation/fpgadataflow/set_folding.py:190: UserWarning: SetFolding doesn't know how to handle op_type StreamingMaxPool_Batch\n",
      "  warnings.warn(\n",
      "/home/julien/finn/src/finn/custom_op/fpgadataflow/streamingmaxpool_batch.py:142: UserWarning: Estimated latency for layer StreamingMaxPool_Batch_0 can be lower than\n",
      "             actual latency!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# automatic setting of folding\n",
    "import finn.transformation.fpgadataflow.set_folding as SetFolding\n",
    "import finn.transformation.fpgadataflow.set_fifo_depths as InsertFIFO\n",
    "from finn.util.basic import pynq_part_map\n",
    "fpga = \"Pynq-Z2\"\n",
    "fpgapart = pynq_part_map[fpga]\n",
    "model = ModelWrapper(build_dir + f\"/{network_name}_dataflow_model.onnx\")\n",
    "#model = model.transform(InsertFIFO.RemoveShallowFIFOs())\n",
    "# model = model.transform(InsertFIFO.InsertFIFO()) # this seems to generate actual hls layers. I am currently of the opinion that these layers actually don't work with the system?\n",
    "model = model.transform(SetFolding.SetFolding(target_cycles_per_frame=folding_amount))\n",
    "model.save(build_dir + f\"/{network_name}_folded.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "404e80ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/home/julien/finn/notebooks/Thesis/TestSynth/Lenet5v9 v10_folding50000_folded.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fceeb77deb0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(build_dir + f\"/{network_name}_folded.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80eda6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/home/julien/finn/notebooks/Thesis/TestSynth/Lenet5v9 v10_folding50000_folded.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fceeb977880>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(build_dir+f\"/{network_name}_folded.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3e4834c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julien/finn/src/finn/transformation/fpgadataflow/floorplan.py:108: UserWarning: 10 nodes have no entry in the provided floorplan, SLR was set to -1\n",
      "  warnings.warn(\n",
      "/home/julien/finn/src/finn/transformation/fpgadataflow/insert_fifo.py:234: UserWarning: Input FIFO for IODMA_0_out0 has depth 2 and won't\n",
      "                        be created. This may cause RTL simulation issues.\n",
      "                        \n",
      "  warnings.warn(\n",
      "/home/julien/finn/src/finn/transformation/fpgadataflow/insert_fifo.py:292: UserWarning: Output FIFO for MatrixVectorActivation_2_out0 has depth 2 and won't\n",
      "                        be created. This may cause RTL simulation issues.\n",
      "                        \n",
      "  warnings.warn(\n",
      "/home/julien/finn/src/finn/transformation/fpgadataflow/create_stitched_ip.py:307: UserWarning: First node is not StreamingFIFO or IODMA.\n",
      "                You may experience incorrect stitched-IP rtlsim or hardware\n",
      "                behavior. It is strongly recommended to insert FIFOs prior to\n",
      "                calling CreateStitchedIP.\n",
      "  warnings.warn(\n",
      "WARNING:root:This is a test warning, here is the information for /tmp/finn_dev_julien/vivado_zynq_proj_sz91rxyc\n"
     ]
    }
   ],
   "source": [
    "from finn.transformation.fpgadataflow.make_zynq_proj import ZynqBuild\n",
    "import finn.transformation.fpgadataflow.prepare_ip as prepare_ip\n",
    "import finn.transformation.fpgadataflow.insert_iodma as insert_iodma\n",
    "pynq_board = \"Pynq-Z2\"\n",
    "target_clk_ns = 10\n",
    "\n",
    "model = ModelWrapper(build_dir+f\"/{network_name}_folded.onnx\")\n",
    "#the best effort generator.\n",
    "model = model.transform(ZynqBuild(platform = pynq_board, period_ns = target_clk_ns)) # handles all the synthesis parts\n",
    "model.save(build_dir+f\"/{network_name}_synthesised.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d670d308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/home/julien/finn/notebooks/Thesis/TestSynth/Lenet5v9 v10_folding50000_synthesised.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fceeb79aa30>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(build_dir+f\"/{network_name}_synthesised.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d365d25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/tmp/finn_dev_julien/dataflow_partition_t7h3emnf/partition_2.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fceeb751a60>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ModelWrapper(build_dir+f\"/{network_name}_synthesised.onnx\")\n",
    "sdp_node_middle = getCustomOp(model.graph.node[1])\n",
    "postsynth_layers = sdp_node_middle.get_nodeattr(\"model\")\n",
    "\n",
    "showInNetron(postsynth_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b284f0",
   "metadata": {},
   "source": [
    "### Recover the amount of resources used\n",
    "This will recover the amount of resources used for each partition after synth is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "807bdcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelWrapper(build_dir+f\"/{network_name}_synthesised.onnx\")\n",
    "from finn.transformation.fpgadataflow.annotate_resources import AnnotateResources\n",
    "model = model.transform(AnnotateResources('synth'))\n",
    "model.save(build_dir+f\"/{network_name}_synthesised_resources.onnx\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4474c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/home/julien/finn/notebooks/Thesis/TestSynth/Lenet5v9 v10_folding50000_synthesised_resources.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fceeb79a310>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " showInNetron(build_dir+f\"/{network_name}_synthesised_resources.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1c5c12",
   "metadata": {},
   "source": [
    "## Generate the Pynq driver zip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7408c5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.fpgadataflow.make_pynq_driver import MakePYNQDriver\n",
    "model = model.transform(MakePYNQDriver(\"zynq-iodma\"))\n",
    "model.save(build_dir + f\"/{network_name}_synth.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "33947b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done Lenet5v9 v10_folding50000\n"
     ]
    }
   ],
   "source": [
    "from shutil import copy\n",
    "from distutils.dir_util import copy_tree\n",
    "\n",
    "# create directory for deployment files\n",
    "deployment_dir = make_build_dir(prefix=\"pynq_deployment_\")\n",
    "model.set_metadata_prop(\"pynq_deployment_dir\", deployment_dir)\n",
    "\n",
    "# get and copy necessary files\n",
    "# .bit and .hwh file\n",
    "bitfile = model.get_metadata_prop(\"bitfile\")\n",
    "hwh_file = model.get_metadata_prop(\"hw_handoff\")\n",
    "deploy_files = [bitfile, hwh_file]\n",
    "\n",
    "for dfile in deploy_files:\n",
    "    if dfile is not None:\n",
    "        copy(dfile, deployment_dir)\n",
    "\n",
    "# driver.py and python libraries\n",
    "pynq_driver_dir = model.get_metadata_prop(\"pynq_driver_dir\")\n",
    "copy_tree(pynq_driver_dir, deployment_dir)\n",
    "\n",
    "from shutil import make_archive\n",
    "make_archive(f'{network_name}', 'zip', deployment_dir)\n",
    "print(f\"done {network_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c2a350",
   "metadata": {},
   "source": [
    "Validating the Accuracy on a PYNQ Board\n",
    "\n",
    "Ensure that your PYNQ board has a working internet connecting for the next steps, since there is some downloading involved.\n",
    "\n",
    "To validate the accuracy, we first need to install the dataset-loading Python package to the PYNQ board. This will give us a convenient way of downloading and accessing the MNIST dataset.\n",
    "\n",
    "We can now use the validate.py script that was generated together with the driver to measure top-1 accuracy on the MNIST dataset.\n",
    "\n",
    "Important to note: override the provided validate.py script with the custom one provided in the root folder.\n",
    "\n",
    "Command to execute on PYNQ board:\n",
    "\n",
    "sudo python3 validate.py --dataset mnist --batchsize 1000\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
