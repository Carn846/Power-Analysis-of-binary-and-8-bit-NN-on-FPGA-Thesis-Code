{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49454ccd",
   "metadata": {},
   "source": [
    "# MNIST\n",
    "This will contain the steps to convert a MNIST Network into the pynq-z2 firmware. Needs to be run in the FINN docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "454f73fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Amount of time that the BNN will have to train is: 0.02 hours\n",
      "Training on: cuda:0\n",
      "Training Class initalised at: 2023-10-06 23:21:07.729684\n",
      "loading dataset: MNIST\n",
      "Dataset loaded\n",
      "Network Accuracy: 90\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "trinary = False # Select the type of network\n",
    "if trinary:\n",
    "    from MNIST import QuantTrinaryFCMNIST\n",
    "    Network = QuantTrinaryFCMNIST()\n",
    "    inputVector = (1,1,28,28)\n",
    "    network_name = \"QuantTrinaryFCMNIST_fast\" # Allow for easy adaptability of the network\n",
    "    PATH = f'/home/julien/finn/notebooks/Thesis/Training_log/MNIST NETWORKS2023-08-20/MNISTv2-2bit.pth' # Ensure Correct Path\n",
    "    Network.load_state_dict(torch.load(PATH)) # THIS WAS COMMENTED OUT\n",
    "    # Checking to see if the network loaded acutally has been trained\n",
    "    # Initialise the network trainer\n",
    "    from NetworkTrainer import Networktrainer\n",
    "    bnnTrainTime = 0.02 # will leave hard coded in.\n",
    "\n",
    "\n",
    "    Trainer = Networktrainer(bnnTrainTime)\n",
    "    Trainer.load_dataset(\"MNIST\")\n",
    "    with open('dumy.txt','w') as dummyLogFile:\n",
    "        print(f'Network Accuracy: {Trainer.Test_Accuracy(Network,dummyLogFile)}')\n",
    "    Network.to('cpu')\n",
    "else:\n",
    "    # the 8 bit one\n",
    "    from MNIST import FC8bit\n",
    "    Network = FC8bit()\n",
    "    inputVector = (1,1,28,28)\n",
    "    network_name = \"FC8bitv4_fast\" # Allow for easy adaptability of the network\n",
    "    PATH = f'/home/julien/finn/notebooks/Thesis/Training_log/MNIST NETWORKS2023-08-20/FC8bit.pth' # Ensure Correct path\n",
    "    Network.load_state_dict(torch.load(PATH)) # THIS WAS COMMENTED OUT\n",
    "    # Checking to see if the network loaded acutally has been trained\n",
    "    # Initialise the network trainer\n",
    "    from NetworkTrainer import Networktrainer\n",
    "    bnnTrainTime = 0.02 # will leave hard coded in.\n",
    "\n",
    "\n",
    "    Trainer = Networktrainer(bnnTrainTime)\n",
    "    Trainer.load_dataset(\"MNIST\")\n",
    "    with open('dumy.txt','w') as dummyLogFile:\n",
    "        print(f'Network Accuracy: {Trainer.Test_Accuracy(Network,dummyLogFile)}')\n",
    "    Network.to('cpu')\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2b79af",
   "metadata": {},
   "source": [
    "## Find Number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e135a340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 25452\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in Network.parameters())\n",
    "print(f\"Number of parameters: {total_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cc148e",
   "metadata": {},
   "source": [
    "## Converting model into Qonnx\n",
    "Will need to export the model into a qonnx version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7fbd658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.util.basic import make_build_dir\n",
    "from finn.util.visualization import showInNetron\n",
    "import os\n",
    "\n",
    "import onnx\n",
    "from finn.util.test import get_test_model_trained\n",
    "import brevitas.onnx as bo\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "from qonnx.transformation.fold_constants import FoldConstants\n",
    "from qonnx.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames, RemoveStaticGraphInputs\n",
    "\n",
    "build_dir = f'{os.getcwd()}/TestSynth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b64c8aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julien/finn/deps/qonnx/src/qonnx/core/modelwrapper.py:93: UserWarning: Some old-style domain attributes were automatically converted to new-style,\n",
      "                i.e. domain=finn to domain=qonnx.custom_op.<general|fpgadataflow|...>\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Saving the network - taken from the demo\n",
    "bo.export_finn_onnx(Network, inputVector, build_dir + f\"/{network_name}_export.onnx\")\n",
    "model = ModelWrapper(build_dir + f\"/{network_name}_export.onnx\")\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(FoldConstants())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "model = model.transform(RemoveStaticGraphInputs())\n",
    "model.save(build_dir + f\"/{network_name}_tidy.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40913b0a",
   "metadata": {},
   "source": [
    "Display the imported qonnx model. No operations have taken place at the moment besides inital set up seen in last block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fd19661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving '/home/julien/finn/notebooks/Thesis/TestSynth/FC8bitv4_fast_tidy.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f51d70f2310>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(build_dir + f\"/{network_name}_tidy.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adebd789",
   "metadata": {},
   "source": [
    "## The pre and post processing steps.\n",
    "### Preprocessing\n",
    "No Preprocessing done in this work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "273c4399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just loading all the used modules\n",
    "from finn.util.pytorch import ToTensor\n",
    "from qonnx.transformation.merge_onnx_models import MergeONNXModels\n",
    "from qonnx.core.datatype import DataType\n",
    "from qonnx.transformation.insert_topk import InsertTopK\n",
    "from qonnx.transformation.infer_datatypes import InferDataTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a5c35ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Although not doing preprocessing will still do the model calls so that if you do use it you can just insert\n",
    "# your instructions here.\n",
    "model = ModelWrapper(build_dir+f\"/{network_name}_tidy.onnx\")\n",
    "# add input quantization annotation: UINT8 for all BNN-PYNQ models\n",
    "global_inp_name = model.graph.input[0].name\n",
    "model.set_tensor_datatype(global_inp_name, DataType[\"UINT8\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d5dc0b",
   "metadata": {},
   "source": [
    "### Post Processing,\n",
    "Inserting a topK layer that will allow the classification to pick a label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2266f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# postprocessing: insert Top-1 node at the end\n",
    "model = model.transform(InsertTopK(k=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6684278",
   "metadata": {},
   "source": [
    "### Tidy up the model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "587f68c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/home/julien/finn/notebooks/Thesis/TestSynth/FC8bitv4_fast_pre_post.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f51d70e3d30>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tidy-up again\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(FoldConstants())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "model = model.transform(InferDataTypes())\n",
    "model = model.transform(RemoveStaticGraphInputs())\n",
    "model.save(build_dir+f\"/{network_name}_pre_post.onnx\")\n",
    "# Show the network again\n",
    "showInNetron(build_dir+f\"/{network_name}_pre_post.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5222ef70",
   "metadata": {},
   "source": [
    "### Streamlining and lowering layers\n",
    "This process is highly dependent on the topography of the network. As such it will differ from each type of network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bae87db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julien/finn/deps/qonnx/src/qonnx/transformation/infer_data_layouts.py:119: UserWarning: Assuming 4D input is NCHW\n",
      "  warnings.warn(\"Assuming 4D input is NCHW\")\n"
     ]
    }
   ],
   "source": [
    "from finn.transformation.streamline import Streamline\n",
    "import finn\n",
    "from qonnx.transformation.lower_convs_to_matmul import LowerConvsToMatMul\n",
    "from qonnx.transformation.bipolar_to_xnor import ConvertBipolarMatMulToXnorPopcount\n",
    "import finn.transformation.streamline.absorb as absorb\n",
    "from finn.transformation.streamline.reorder import MakeMaxPoolNHWC, MoveScalarLinearPastInvariants\n",
    "from qonnx.transformation.infer_data_layouts import InferDataLayouts\n",
    "from qonnx.transformation.general import RemoveUnusedTensors\n",
    "\n",
    "model = ModelWrapper(build_dir + f\"/{network_name}_pre_post.onnx\")\n",
    "model = model.transform(MoveScalarLinearPastInvariants())\n",
    "model = model.transform(Streamline())\n",
    "model = model.transform(LowerConvsToMatMul())\n",
    "model = model.transform(MakeMaxPoolNHWC())\n",
    "model = model.transform(absorb.AbsorbTransposeIntoMultiThreshold())\n",
    "model = model.transform(ConvertBipolarMatMulToXnorPopcount())\n",
    "model = model.transform(Streamline())\n",
    "# absorb final add-mul nodes into TopK\n",
    "model = model.transform(absorb.AbsorbScalarMulAddIntoTopK())\n",
    "model = model.transform(InferDataLayouts())\n",
    "model = model.transform(RemoveUnusedTensors())\n",
    "model.save(build_dir+f\"/{network_name}_streamlined.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46d918c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/home/julien/finn/notebooks/Thesis/TestSynth/FC8bitv4_fast_streamlined.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f51d70f2df0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(build_dir+f\"/{network_name}_streamlined.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d748afe",
   "metadata": {},
   "source": [
    "### Converting the layers into the HW equivalent\n",
    "\n",
    "It is this stage which will be the hardest. I will need to ensure that each node is able to be converted a compatable version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6da0e145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import finn.transformation.fpgadataflow.convert_to_hls_layers as to_hls\n",
    "from finn.transformation.fpgadataflow.create_dataflow_partition import (\n",
    "    CreateDataflowPartition,\n",
    ")\n",
    "import finn.builder.build_dataflow \n",
    "from finn.transformation.move_reshape import RemoveCNVtoFCFlatten\n",
    "from qonnx.custom_op.registry import getCustomOp\n",
    "from qonnx.transformation.infer_data_layouts import InferDataLayouts\n",
    "\n",
    "# choose the memory mode for the MVTU units, decoupled or const\n",
    "mem_mode = \"decoupled\" # smaller memory foot print. Longer synth times use 'const'\n",
    "\n",
    "model = ModelWrapper(build_dir + f\"/{network_name}_streamlined.onnx\")\n",
    "model = model.transform(to_hls.InferBinaryMatrixVectorActivation(mem_mode))\n",
    "\n",
    "model = model.transform(to_hls.InferQuantizedMatrixVectorActivation(mem_mode))\n",
    "# TopK to LabelSelect\n",
    "model = model.transform(to_hls.InferLabelSelectLayer())\n",
    "# input quantization (if any) to standalone thresholding\n",
    "model = model.transform(to_hls.InferThresholdingLayer())\n",
    "model = model.transform(to_hls.InferConvInpGen())\n",
    "model = model.transform(to_hls.InferStreamingMaxPool())\n",
    "# get rid of Reshape(-1, 1) operation between hlslib nodes\n",
    "#model = model.transform(RemoveCNVtoFCFlatten()) # comment out when not using any conv layers\n",
    "# get rid of Tranpose -> Tranpose identity seq\n",
    "model = model.transform(absorb.AbsorbConsecutiveTransposes())\n",
    "# infer tensor data layouts\n",
    "model = model.transform(InferDataLayouts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50b0f641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partitioning the network\n",
    "parent_model = model.transform(CreateDataflowPartition())\n",
    "parent_model.save(build_dir + f\"/{network_name}_dataflow_parent.onnx\")\n",
    "sdp_node = parent_model.get_nodes_by_op_type(\"StreamingDataflowPartition\")[0]\n",
    "#print(sdp_node)\n",
    "sdp_node = getCustomOp(sdp_node)\n",
    "dataflow_model_filename = sdp_node.get_nodeattr(\"model\")\n",
    "# save the dataflow partition with a different name for easier access\n",
    "dataflow_model = ModelWrapper(dataflow_model_filename)\n",
    "dataflow_model.save(build_dir + f\"/{network_name}_dataflow_model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9c61e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/home/julien/finn/notebooks/Thesis/TestSynth/FC8bitv4_fast_dataflow_model.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f51d6ce5fa0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(build_dir + f\"/{network_name}_dataflow_model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7fc5cacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/home/julien/finn/notebooks/Thesis/TestSynth/FC8bitv4_fast_dataflow_parent.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f51d7025e20>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(build_dir + f\"/{network_name}_dataflow_parent.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ba7bda",
   "metadata": {},
   "source": [
    "# Folding the network\n",
    "Uses the c++ synthesis tool to determine folding settings. Not optimal but will be procedual for this thesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5475ff88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julien/finn/src/finn/transformation/fpgadataflow/insert_fifo.py:234: UserWarning: Input FIFO for Reshape_0_out0 has depth 2 and won't\n",
      "                        be created. This may cause RTL simulation issues.\n",
      "                        \n",
      "  warnings.warn(\n",
      "/home/julien/finn/src/finn/transformation/fpgadataflow/insert_fifo.py:292: UserWarning: Output FIFO for global_out has depth 2 and won't\n",
      "                        be created. This may cause RTL simulation issues.\n",
      "                        \n",
      "  warnings.warn(\n",
      "/home/julien/finn/src/finn/transformation/fpgadataflow/set_folding.py:204: UserWarning: Node MatrixVectorActivation_0 is bottleneck with 196 cycles, running second pass\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# automatic setting of folding\n",
    "import finn.transformation.fpgadataflow.set_folding as SetFolding\n",
    "import finn.transformation.fpgadataflow.set_fifo_depths as InsertFIFO\n",
    "from finn.util.basic import pynq_part_map\n",
    "fpga = \"Pynq-Z2\"\n",
    "fpgapart = pynq_part_map[fpga]\n",
    "model = ModelWrapper(build_dir + f\"/{network_name}_dataflow_model.onnx\")\n",
    "#model = model.transform(InsertFIFO.RemoveShallowFIFOs())\n",
    "model = model.transform(SetFolding.SetFolding(target_cycles_per_frame=1))\n",
    "model.save(build_dir + f\"/{network_name}_folded.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "404e80ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/home/julien/finn/notebooks/Thesis/TestSynth/FC8bitv4_fast_folded.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f51d703c4f0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(build_dir + f\"/{network_name}_folded.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80eda6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/home/julien/finn/notebooks/Thesis/TestSynth/FC8bitv4_fast_folded.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f51d6ce5f70>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(build_dir+f\"/{network_name}_folded.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3e50b7",
   "metadata": {},
   "source": [
    "## Synthesis the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3e4834c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julien/finn/src/finn/transformation/fpgadataflow/floorplan.py:108: UserWarning: 6 nodes have no entry in the provided floorplan, SLR was set to -1\n",
      "  warnings.warn(\n",
      "/home/julien/finn/src/finn/transformation/fpgadataflow/insert_fifo.py:234: UserWarning: Input FIFO for IODMA_0_out0 has depth 2 and won't\n",
      "                        be created. This may cause RTL simulation issues.\n",
      "                        \n",
      "  warnings.warn(\n",
      "/home/julien/finn/src/finn/transformation/fpgadataflow/insert_fifo.py:292: UserWarning: Output FIFO for LabelSelect_Batch_0_out0 has depth 2 and won't\n",
      "                        be created. This may cause RTL simulation issues.\n",
      "                        \n",
      "  warnings.warn(\n",
      "/home/julien/finn/src/finn/transformation/fpgadataflow/create_stitched_ip.py:307: UserWarning: First node is not StreamingFIFO or IODMA.\n",
      "                You may experience incorrect stitched-IP rtlsim or hardware\n",
      "                behavior. It is strongly recommended to insert FIFOs prior to\n",
      "                calling CreateStitchedIP.\n",
      "  warnings.warn(\n",
      "WARNING:root:This is a test warning, here is the information for /tmp/finn_dev_julien/vivado_zynq_proj_xbzkimxw\n",
      "ERROR: [Common 17-69] Command failed: Run 'impl_1' failed. Unable to open\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Synthesis failed, no bitfile found. Check logs under /tmp/finn_dev_julien/vivado_zynq_proj_xbzkimxw",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m ModelWrapper(build_dir\u001b[38;5;241m+\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnetwork_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_folded.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#the best effort generator. Doesn't seem to want to work for some reason.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#model = model.transform(insert_iodma.InsertIODMA()) # Handled by below code\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mZynqBuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplatform\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpynq_board\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperiod_ns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtarget_clk_ns\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# handles all the synthesis parts\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#model = model.transform(prepare_ip.PrepareIP(fpgapart=fpgapart,clk=target_clk_ns)) #create for custom blocks. Not needed here\u001b[39;00m\n\u001b[1;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39msave(build_dir\u001b[38;5;241m+\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnetwork_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_synthesised.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/julien/finn/deps/qonnx/src/qonnx/core/modelwrapper.py:140\u001b[0m, in \u001b[0;36mModelWrapper.transform\u001b[0;34m(self, transformation, make_deepcopy, cleanup)\u001b[0m\n\u001b[1;32m    138\u001b[0m model_was_changed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m model_was_changed:\n\u001b[0;32m--> 140\u001b[0m     (transformed_model, model_was_changed) \u001b[38;5;241m=\u001b[39m \u001b[43mtransformation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformed_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cleanup:\n\u001b[1;32m    142\u001b[0m     transformed_model\u001b[38;5;241m.\u001b[39mcleanup()\n",
      "File \u001b[0;32m/home/julien/finn/src/finn/transformation/fpgadataflow/make_zynq_proj.py:367\u001b[0m, in \u001b[0;36mZynqBuild.apply\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    365\u001b[0m     kernel_model\u001b[38;5;241m.\u001b[39msave(dataflow_model_filename)\n\u001b[1;32m    366\u001b[0m \u001b[38;5;66;03m# Assemble design from IPs\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mMakeZYNQProject\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplatform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_debug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menable_debug\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# set platform attribute for correct remote execution\u001b[39;00m\n\u001b[1;32m    372\u001b[0m model\u001b[38;5;241m.\u001b[39mset_metadata_prop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplatform\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzynq-iodma\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/julien/finn/deps/qonnx/src/qonnx/core/modelwrapper.py:140\u001b[0m, in \u001b[0;36mModelWrapper.transform\u001b[0;34m(self, transformation, make_deepcopy, cleanup)\u001b[0m\n\u001b[1;32m    138\u001b[0m model_was_changed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m model_was_changed:\n\u001b[0;32m--> 140\u001b[0m     (transformed_model, model_was_changed) \u001b[38;5;241m=\u001b[39m \u001b[43mtransformation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformed_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cleanup:\n\u001b[1;32m    142\u001b[0m     transformed_model\u001b[38;5;241m.\u001b[39mcleanup()\n",
      "File \u001b[0;32m/home/julien/finn/src/finn/transformation/fpgadataflow/make_zynq_proj.py:278\u001b[0m, in \u001b[0;36mMakeZYNQProject.apply\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    274\u001b[0m bitfile_name \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    275\u001b[0m     vivado_pynq_proj_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/finn_zynq_link.runs/impl_1/top_wrapper.bit\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m )\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(bitfile_name):\n\u001b[0;32m--> 278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSynthesis failed, no bitfile found. Check logs under \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;241m%\u001b[39m vivado_pynq_proj_dir\n\u001b[1;32m    281\u001b[0m     )\n\u001b[1;32m    282\u001b[0m deploy_bitfile_name \u001b[38;5;241m=\u001b[39m vivado_pynq_proj_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/resizer.bit\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m copy(bitfile_name, deploy_bitfile_name)\n",
      "\u001b[0;31mException\u001b[0m: Synthesis failed, no bitfile found. Check logs under /tmp/finn_dev_julien/vivado_zynq_proj_xbzkimxw"
     ]
    }
   ],
   "source": [
    "from finn.transformation.fpgadataflow.make_zynq_proj import ZynqBuild\n",
    "import finn.transformation.fpgadataflow.prepare_ip as prepare_ip\n",
    "import finn.transformation.fpgadataflow.insert_iodma as insert_iodma\n",
    "pynq_board = \"Pynq-Z2\"\n",
    "target_clk_ns = 10\n",
    "\n",
    "model = ModelWrapper(build_dir+f\"/{network_name}_folded.onnx\")\n",
    "model = model.transform(ZynqBuild(platform = pynq_board, period_ns = target_clk_ns)) # handles all the synthesis parts\n",
    "model.save(build_dir+f\"/{network_name}_synthesised.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d670d308",
   "metadata": {},
   "outputs": [],
   "source": [
    "showInNetron(build_dir+f\"/{network_name}_synthesised.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed01d73b",
   "metadata": {},
   "source": [
    "### Recover the amount of resources used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d365d25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelWrapper(build_dir+f\"/{network_name}_synthesised.onnx\")\n",
    "sdp_node_middle = getCustomOp(model.graph.node[1])\n",
    "postsynth_layers = sdp_node_middle.get_nodeattr(\"model\")\n",
    "\n",
    "showInNetron(postsynth_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242d2b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelWrapper(build_dir+f\"/{network_name}_synthesised.onnx\")\n",
    "from finn.transformation.fpgadataflow.annotate_resources import AnnotateResources\n",
    "model = model.transform(AnnotateResources('synth'))\n",
    "model.save(build_dir+f\"/{network_name}_synthesised_resources.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bb0950",
   "metadata": {},
   "outputs": [],
   "source": [
    " showInNetron(build_dir+f\"/{network_name}_synthesised_resources.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccab010",
   "metadata": {},
   "source": [
    "## Make the pynq driver zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7408c5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.fpgadataflow.make_pynq_driver import MakePYNQDriver\n",
    "model = model.transform(MakePYNQDriver(\"zynq-iodma\"))\n",
    "model.save(build_dir + f\"/{network_name}_synth.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33947b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import copy\n",
    "from distutils.dir_util import copy_tree\n",
    "\n",
    "# create directory for deployment files\n",
    "deployment_dir = make_build_dir(prefix=\"pynq_deployment_\")\n",
    "model.set_metadata_prop(\"pynq_deployment_dir\", deployment_dir)\n",
    "\n",
    "# get and copy necessary files\n",
    "# .bit and .hwh file\n",
    "bitfile = model.get_metadata_prop(\"bitfile\")\n",
    "hwh_file = model.get_metadata_prop(\"hw_handoff\")\n",
    "deploy_files = [bitfile, hwh_file]\n",
    "\n",
    "for dfile in deploy_files:\n",
    "    if dfile is not None:\n",
    "        copy(dfile, deployment_dir)\n",
    "\n",
    "# driver.py and python libraries\n",
    "pynq_driver_dir = model.get_metadata_prop(\"pynq_driver_dir\")\n",
    "copy_tree(pynq_driver_dir, deployment_dir)\n",
    "\n",
    "from shutil import make_archive\n",
    "make_archive(f'{network_name}', 'zip', deployment_dir)\n",
    "print(f\"done {network_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c2a350",
   "metadata": {},
   "source": [
    "Validating the Accuracy on a PYNQ Board\n",
    "\n",
    "Ensure that your PYNQ board has a working internet connecting for the next steps, since there is some downloading involved.\n",
    "\n",
    "We can now use the validate.py script that was generated together with the driver to measure top-1 accuracy on the MNIST dataset.\n",
    "\n",
    "Important: override the provided FINN validate.py with one provided in root.\n",
    "\n",
    "Command to execute on PYNQ board:\n",
    "\n",
    "sudo python3 validate.py --dataset mnist --batchsize 1000\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
